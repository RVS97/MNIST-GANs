{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TCV2 - InceptionNet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RVS97/MNIST-GANs/blob/master/TCV2_InceptionNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5Sbwv60G9Ip",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqLI2G4dHA_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "from tensorflow.python import keras\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.keras.utils import to_categorical\n",
        "from tensorflow.python.keras.optimizers import Adam, SGD\n",
        "from tensorflow.python.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.python.keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, Reshape\n",
        "from tensorflow.python.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Conv2DTranspose \n",
        "from tensorflow.python.keras.layers import Input, UpSampling2D, concatenate  \n",
        "from tensorflow.python.keras.layers import LeakyReLU\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "from sklearn.utils import shuffle\n",
        "from matplotlib import pyplot\n",
        "from math import ceil\n",
        "from scipy.stats import describe\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import pickle\n",
        "import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eODII4eNFi3S",
        "colab_type": "text"
      },
      "source": [
        "# Dataset\n",
        "Import MNIST dataset from keras and map image values to 0/1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhSxSV6TFd6E",
        "colab_type": "code",
        "outputId": "82c2e3b2-2149-4dd0-c28e-3def14816067",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Import dataset\n",
        "from keras.datasets import mnist\n",
        "\n",
        "class MNISTdata:\n",
        "  def __init__(self, batchSize=100, shuffle=True, vBatchNorm=False):\n",
        "    \n",
        "    # Load into variables\n",
        "    (self.x_train, self.y_train),(self.x_test, self.y_test) = mnist.load_data()\n",
        "\n",
        "    self.x_train = np.expand_dims(self.x_train, -1)\n",
        "    self.x_test = np.expand_dims(self.x_test, -1)\n",
        "    \n",
        "    # One-hot encode labels\n",
        "    self.y_train = to_categorical(self.y_train, num_classes=10)\n",
        "    self.y_test = to_categorical(self.y_test, num_classes=10)\n",
        "    \n",
        "    # Map image values to the range -1/1\n",
        "    self.x_train = (self.x_train.astype(np.float32) - 127.5)/127.5*-1\n",
        "    self.x_test = (self.x_test.astype(np.float32) - 127.5)/127.5*-1\n",
        "\n",
        "    self.imgWidth = len(self.x_train[0][0])\n",
        "    self.imgHeight = len(self.x_train[0])\n",
        "    self.nTrainSamples = len(self.x_train)\n",
        "    self.nTestSamples = len(self.x_test)\n",
        "    \n",
        "    print(\"MNIST loaded correctly\")\n",
        "    print(\" - {} by {} images (grayscale)\".format(self.imgWidth,self.imgHeight))\n",
        "    print(\" - {} training samples\".format(self.nTrainSamples))\n",
        "    print(\" - {} test samples\".format(self.nTestSamples))\n",
        "    \n",
        "    self.batchSize = batchSize\n",
        "    self.nBatches = ceil(self.nTrainSamples/self.batchSize)\n",
        "    \n",
        "    print(\"Batch size {} -> {} batches\".format(self.batchSize, self.nBatches))\n",
        "    \n",
        "    if shuffle: self.shuffleData()\n",
        "    \n",
        "  def shuffleData(self):\n",
        "    self.x_train, self.y_train = shuffle(self.x_train, self.y_train)\n",
        "    \n",
        "  def getBatch(self, batchId):\n",
        "    if (batchId+1)*self.batchSize > self.nTrainSamples:\n",
        "      batch = self.x_train[batchId*self.batchSize:]\n",
        "    else:\n",
        "      batch = self.x_train[batchId*self.batchSize:(batchId+1)*self.batchSize]\n",
        "    return batch, np.ones((len(batch),))    \n",
        "    \n",
        "data = MNISTdata(batchSize = 200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MNIST loaded correctly\n",
            " - 28 by 28 images (grayscale)\n",
            " - 60000 training samples\n",
            " - 10000 test samples\n",
            "Batch size 200 -> 300 batches\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA0Ob08XF2_N",
        "colab_type": "text"
      },
      "source": [
        "# Google Drive Mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klo5pF6EF4aJ",
        "colab_type": "code",
        "outputId": "2acf8bf0-d67f-43c0-80d2-c1cc20e4e292",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "rootDir = \"/content/rootDir\"\n",
        "\n",
        "def saveFile(filePath, rootDir):\n",
        "  !cp $filePath $rootDir\n",
        "  \n",
        "def getFile(fileName, rootDir, localDir = \"./\"):\n",
        "  path = rootDir + \"/\" + fileName\n",
        "  !cp $path $localDir"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pumeLt-tF65h",
        "colab_type": "text"
      },
      "source": [
        "# Classification Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7wVrJANF6Gg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Simple Network is enough\n",
        "\n",
        "def getClassifier(inputShape=(28,28,1)):\n",
        "  model = Sequential(name=\"Discriminator\")\n",
        "  model.add(Conv2D(32, (3, 3), strides=(2, 2), activation='relu', padding='same', input_shape=inputShape, kernel_initializer='glorot_uniform'))\n",
        "  model.add(BatchNormalization(axis=-1))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), strides=(2, 2), activation='relu', padding='same', kernel_initializer='glorot_uniform'))\n",
        "  model.add(BatchNormalization(axis=-1))\n",
        "  \n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(50, activation='relu', kernel_initializer='glorot_uniform'))\n",
        "  model.add(BatchNormalization(axis=-1))\n",
        "\n",
        "  model.add(Dense(10, activation='sigmoid', kernel_initializer='glorot_uniform'))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJPgQmOSG0dG",
        "colab_type": "text"
      },
      "source": [
        "# Train Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E4_FGSCGz1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classNet = getClassifier()\n",
        "classNet.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "hist = classNet.fit(data.x_train, data.y_train, batch_size=200, epochs=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O92T8x39JIfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pyplot.plot(hist.history['loss'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az7OZQ77kKTh",
        "colab_type": "text"
      },
      "source": [
        "#Evaluate Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW7Go3AoIlvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classNet.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "testEval = classNet.evaluate(data.x_test, data.y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}